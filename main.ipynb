{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "df.columns = ['Datatime', 'temp_day', 'wind', 'pressure_day', 'temp_even', 'pressure_even']  \n",
    "df = df.drop(columns = ['wind', 'pressure_day', 'temp_even', 'pressure_even'])\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "df.replace(\" \", nan_value, inplace=True)\n",
    "df = df.dropna()\n",
    "\n",
    "df[['temp_day']] = df[['temp_day']].astype(int) \n",
    "df_save = df\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig1 = plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Temperature graph in Celsius\")\n",
    "plt.xlabel(\"Day\")\n",
    "plt.ylabel(\"Temp\")\n",
    "plt.plot(df['temp_day'], color=\"green\")\n",
    "plt.legend([\"Temp in day\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_lags(df: pd.DataFrame, n_lags: int):\n",
    "    \"\"\"the function of generating observations with a time delay\"\"\"\n",
    "    df_n = df.copy()\n",
    "    for n in range(1, n_lags + 1):\n",
    "        df_n[f\"lag{n}\"] = df_n[\"value\"].shift(n)\n",
    "    df_n = df_n.iloc[n_lags:]\n",
    "    return df_n\n",
    "\n",
    "input_dim = 100\n",
    "\n",
    "df_timelags = generate_time_lags(df, input_dim)\n",
    "print(df_timelags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def feature_label_split(df: pd.DataFrame, target_col: str):\n",
    "    \"\"\"separtion into x and y\"\"\"\n",
    "    y = df[[target_col]]\n",
    "    X = df.drop(columns=[target_col])\n",
    "    return X, y\n",
    "\n",
    "def train_val_test_split(df: pd.DataFrame, target_col: str, test_ratio: float) -> pd.DataFrame:\n",
    "    \"\"\"separtion into test, val, test\"\"\"\n",
    "\n",
    "    val_ratio = test_ratio / (1 - test_ratio)\n",
    "\n",
    "    X, y = feature_label_split(df, target_col)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_ratio, shuffle=False)  \n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(df_timelags, 'value', 0.1)\n",
    "\n",
    "print(f'Процент обучащей выборки: {X_train[\"lag1\"].count() / df_timelags[\"lag1\"].count()*100}%')\n",
    "print(f'Процент тестовой выборки: {X_test[\"lag1\"].count() / df_timelags[\"lag1\"].count()*100}%')\n",
    "print(f'Процент валидационной выборки: {X_val[\"lag1\"].count() / df_timelags[\"lag1\"].count()*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "def get_scaler(scaler: str) -> sklearn.preprocessing:\n",
    "    scalers = {\n",
    "        \"minmax\": MinMaxScaler,\n",
    "        \"standard\": StandardScaler,\n",
    "        \"maxabs\": MaxAbsScaler,\n",
    "        \"robust\": RobustScaler,\n",
    "    }\n",
    "    return scalers.get(scaler.lower())()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = get_scaler('minmax')\n",
    "\n",
    "X_train_arr = scaler.fit_transform(X_train)\n",
    "X_val_arr = scaler.transform(X_val)\n",
    "X_test_arr = scaler.transform(X_test)\n",
    "\n",
    "y_train_arr = scaler.fit_transform(y_train)\n",
    "y_val_arr = scaler.transform(y_val)\n",
    "y_test_arr = scaler.transform(y_test)   \n",
    "\n",
    "print(X_train_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_features = torch.Tensor(X_train_arr)\n",
    "train_targets = torch.Tensor(y_train_arr)\n",
    "val_features = torch.Tensor(X_val_arr)\n",
    "val_targets = torch.Tensor(y_val_arr)\n",
    "test_features = torch.Tensor(X_test_arr)\n",
    "test_targets = torch.Tensor(y_test_arr)\n",
    "\n",
    "train = TensorDataset(train_features, train_targets)\n",
    "val = TensorDataset(val_features, val_targets)\n",
    "test = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "test_loader_one = DataLoader(test, batch_size=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, layer_dim: int, output_dim: int, dropout_prob: float) -> None:\n",
    "        \"\"\"The __init__ method that initiates an RNN instance.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): The number of nodes in the input layer\n",
    "            hidden_dim (int): The number of nodes in each layer\n",
    "            layer_dim (int): The number of layers in the network\n",
    "            output_dim (int): The number of nodes in the output layer\n",
    "            dropout_prob (float): The probability of nodes being dropped out\n",
    "\n",
    "        \"\"\"\n",
    "        super(RNNModel, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # RNN layers\n",
    "        self.rnn = nn.RNN(\n",
    "            input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob\n",
    "        )\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"The forward method takes input tensor x and does forward propagation\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of the shape (batch size, sequence length, input_dim)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of the shape (batch size, output_dim)\n",
    "\n",
    "        \"\"\"\n",
    "        # Initializing hidden state for first input with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Forward propagation by passing in the input and hidden state into the model\n",
    "        out, h0 = self.rnn(x, h0.detach())\n",
    "\n",
    "        # Reshaping the outputs in the shape of (batch_size, seq_length, hidden_size)\n",
    "        # so that it can fit into the fully connected layer\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Convert the final state to our desired output shape (batch_size, output_dim)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimization:\n",
    "    \"\"\"Optimization is a helper class that allows training, validation, prediction.\n",
    "\n",
    "    Optimization is a helper class that takes model, loss function, optimizer function\n",
    "    learning scheduler (optional), early stopping (optional) as inputs. In return, it\n",
    "    provides a framework to train and validate the models, and to predict future values\n",
    "    based on the models.\n",
    "\n",
    "    Attributes:\n",
    "        model (RNNModel, LSTMModel, GRUModel): Model class created for the type of RNN\n",
    "        loss_fn (torch.nn.modules.Loss): Loss function to calculate the losses\n",
    "        optimizer (torch.optim.Optimizer): Optimizer function to optimize the loss function\n",
    "        train_losses (list[float]): The loss values from the training\n",
    "        val_losses (list[float]): The loss values from the validation\n",
    "        last_epoch (int): The number of epochs that the models is trained\n",
    "    \"\"\"\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (RNNModel, LSTMModel, GRUModel): Model class created for the type of RNN\n",
    "            loss_fn (torch.nn.modules.Loss): Loss function to calculate the losses\n",
    "            optimizer (torch.optim.Optimizer): Optimizer function to optimize the loss function\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "    def train_step(self, x, y):\n",
    "        \"\"\"The method train_step completes one step of training.\n",
    "\n",
    "        Given the features (x) and the target values (y) tensors, the method completes\n",
    "        one step of the training. First, it activates the train mode to enable back prop.\n",
    "        After generating predicted values (yhat) by doing forward propagation, it calculates\n",
    "        the losses by using the loss function. Then, it computes the gradients by doing\n",
    "        back propagation and updates the weights by calling step() function.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor for features to train one step\n",
    "            y (torch.Tensor): Tensor for target values to calculate losses\n",
    "\n",
    "        \"\"\"\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()\n",
    "\n",
    "    def train(self, train_loader, val_loader, batch_size=64, n_epochs=50, n_features=1):\n",
    "        \"\"\"The method train performs the model training\n",
    "\n",
    "        The method takes DataLoaders for training and validation datasets, batch size for\n",
    "        mini-batch training, number of epochs to train, and number of features as inputs.\n",
    "        Then, it carries out the training by iteratively calling the method train_step for\n",
    "        n_epochs times. If early stopping is enabled, then it  checks the stopping condition\n",
    "        to decide whether the training needs to halt before n_epochs steps. Finally, it saves\n",
    "        the model in a designated file path.\n",
    "\n",
    "        Args:\n",
    "            train_loader (torch.utils.data.DataLoader): DataLoader that stores training data\n",
    "            val_loader (torch.utils.data.DataLoader): DataLoader that stores validation data\n",
    "            batch_size (int): Batch size for mini-batch training\n",
    "            n_epochs (int): Number of epochs, i.e., train steps, to train\n",
    "            n_features (int): Number of feature columns\n",
    "\n",
    "        \"\"\"\n",
    "        model_path = f'{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            batch_losses = []\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch = x_batch.view([batch_size, -1, n_features]).to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                loss = self.train_step(x_batch, y_batch)\n",
    "                batch_losses.append(loss)\n",
    "            training_loss = np.mean(batch_losses)\n",
    "            self.train_losses.append(training_loss)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_val_losses = []\n",
    "                for x_val, y_val in val_loader:\n",
    "                    x_val = x_val.view([batch_size, -1, n_features]).to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "                    self.model.eval()\n",
    "                    yhat = self.model(x_val)\n",
    "                    val_loss = self.loss_fn(y_val, yhat).item()\n",
    "                    batch_val_losses.append(val_loss)\n",
    "                validation_loss = np.mean(batch_val_losses)\n",
    "                self.val_losses.append(validation_loss)\n",
    "\n",
    "            if (epoch <= 10) | (epoch % 50 == 0):\n",
    "                print(\n",
    "                    f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\"\n",
    "                )\n",
    "\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "    def evaluate(self, test_loader, batch_size=1, n_features=1):\n",
    "        \"\"\"The method evaluate performs the model evaluation\n",
    "\n",
    "        The method takes DataLoaders for the test dataset, batch size for mini-batch testing,\n",
    "        and number of features as inputs. Similar to the model validation, it iteratively\n",
    "        predicts the target values and calculates losses. Then, it returns two lists that\n",
    "        hold the predictions and the actual values.\n",
    "\n",
    "        Note:\n",
    "            This method assumes that the prediction from the previous step is available at\n",
    "            the time of the prediction, and only does one-step prediction into the future.\n",
    "\n",
    "        Args:\n",
    "            test_loader (torch.utils.data.DataLoader): DataLoader that stores test data\n",
    "            batch_size (int): Batch size for mini-batch training\n",
    "            n_features (int): Number of feature columns\n",
    "\n",
    "        Returns:\n",
    "            list[float]: The values predicted by the model\n",
    "            list[float]: The actual values in the test set.\n",
    "\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            values = []\n",
    "            for x_test, y_test in test_loader:\n",
    "                x_test = x_test.view([batch_size, -1, n_features]).to(device)\n",
    "                y_test = y_test.to(device)\n",
    "                self.model.eval()\n",
    "                yhat = self.model(x_test)\n",
    "                predictions.append(yhat.to(device).detach().numpy())\n",
    "                values.append(y_test.to(device).detach().numpy())\n",
    "\n",
    "        return predictions, values\n",
    "\n",
    "    def plot_losses(self):\n",
    "        \"\"\"The method plots the calculated loss values for training and validation\n",
    "        \"\"\"\n",
    "        plt.plot(self.train_losses, label=\"Training loss\")\n",
    "        plt.plot(self.val_losses, label=\"Validation loss\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Losses\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "input_dim = len(X_train.columns)\n",
    "output_dim = 1\n",
    "hidden_dim = 64\n",
    "layer_dim = 3\n",
    "batch_size = 64\n",
    "dropout = 0.2\n",
    "n_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-6\n",
    "\n",
    "model_params = {'input_dim': input_dim,\n",
    "                'hidden_dim' : hidden_dim,\n",
    "                'layer_dim' : layer_dim,\n",
    "                'output_dim' : output_dim,\n",
    "                'dropout_prob' : dropout}\n",
    "\n",
    "model = RNNModel(**model_params)\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction=\"mean\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
    "opt.train(train_loader, val_loader, batch_size=batch_size, n_epochs=n_epochs, n_features=input_dim)\n",
    "opt.plot_losses()\n",
    "\n",
    "predictions, values = opt.evaluate(\n",
    "    test_loader_one,\n",
    "    batch_size=1,\n",
    "    n_features=input_dim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform( scaler: sklearn.preprocessing._data.MinMaxScaler, df: pd.DataFrame, columns: list) -> pd.DataFrame:\n",
    "    \"\"\"function inverse_transform DataFrame\"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = scaler.inverse_transform(df[col])\n",
    "    return df\n",
    "\n",
    "\n",
    "def format_predictions(predictions: list, values: list, df_test: pd.DataFrame, scaler: sklearn.preprocessing._data.MinMaxScaler)-> pd.DataFrame:\n",
    "    \"\"\"DataFrame creation function with predictions\"\"\"\n",
    "    vals = np.concatenate(values, axis=0).ravel()\n",
    "    preds = np.concatenate(predictions, axis=0).ravel()\n",
    "    df_result = pd.DataFrame(data={\"value\": vals, \"prediction\": preds}, index=df_test.head(len(vals)).index)\n",
    "    df_result = df_result.sort_index()\n",
    "    df_result = inverse_transform(scaler, df_result, [[\"value\", \"prediction\"]])\n",
    "    return df_result\n",
    "\n",
    "df_result = format_predictions(predictions, values, X_test, scaler)\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def calculate_metrics(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Function calculate metrics value and predictions\"\"\"\n",
    "    result_metrics = {'mae' : mean_absolute_error(df.value, df.prediction),\n",
    "                      'rmse' : mean_squared_error(df.value, df.prediction) ** 0.5,\n",
    "                      'r2' : r2_score(df.value, df.prediction)}\n",
    "    \n",
    "    print(\"Mean Absolute Error:       \", result_metrics[\"mae\"])   #средняя абсолютная ошибка\n",
    "    print(\"Root Mean Squared Error:   \", result_metrics[\"rmse\"])  #средняя квадратичная ошибка\n",
    "    print(\"R^2 Score:                 \", result_metrics[\"r2\"])    # КФ\n",
    "    return result_metrics\n",
    "\n",
    "result_metrics = calculate_metrics(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison(df_result: pd.DataFrame, month: int, year: int) -> None:\n",
    "    \"\"\"function comparison of a predicate and a DataSet\"\"\"\n",
    "\n",
    "    if (month < 10): month = \"0\" + str(month)\n",
    "    data1 = str(str(year) + \"-\" + month + \"-\" + \"01\")\n",
    "    data2 = str(str(year) + \"-\" + month + \"-\" + \"31\")\n",
    "\n",
    "    df_result = df_result.loc[(df_result[\"Datatime\"] >= data1) & (df_result[\"Datatime\"] <= data2)]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.title(\"Temperature graph in Celsius by month\")  # Fahrenheit temperature graph\n",
    "    df_result = df_result.set_index(['Datatime'])\n",
    "    plt.xlabel(\"Day\")\n",
    "    plt.ylabel(\"Temp\")\n",
    "    plt.plot(df_result['value'], color=\"blue\", marker=\"x\", linewidth=2, markersize=4)\n",
    "    plt.plot(df_result['prediction'], color=\"orange\", marker=\"x\", linewidth=2, markersize=4)\n",
    "    \n",
    "    plt.legend([\"Temp\", \"Prediction temp\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month, year = 1, 2022\n",
    "df_result.reset_index(drop = False, inplace=True)\n",
    "comparison(df_result, month, year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90dccde7fa88626667c2c5661528e62af9bc7fcc5322553539e8646401baa1a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
